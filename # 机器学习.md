# 机器学习
 mismatch  
 overfitting  
 gradient  
 optimization  
 loss  
 local minima  
 global minima  
 saddle point  
 critical point  
 
 ## gradient calculate
 x=torch.tensor  
 z=x.pow(2).sum()  
 z.backward()  
 x.grad  

 ## types
 Regression  
 Classification  
 Structured Learning
 
Function  
Loss,erro surface  
optimization  

Piecewise Linear Curves  
Sigmoid  
$$
y=c\frac{1}{1+e^{-(b+wx_1)}}
$$
$$
y=b+\sum_i c_isigmoid(b_i+\sum_j(w_{ij}x_j))
$$
